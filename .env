QWEN_API_KEY=sk-ccde453981a14a3f8804c42f14e3943e
GEMINI_API_KEY=AIzaSyAjeyKzupKzILwQ9-HJTR5qBIgImNdqldM
LOCAL_MODEL=deepseek-coder
HUGGINGFACE_API_TOKEN=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
HUGGINGFACE_MODEL_URL=
HUGGINGFACE_ROUTER_MODEL=deepseek-ai/DeepSeek-OCR:novita
HUGGINGFACE_ROUTER_MODEL=meta-llama/Llama-3.1-8B-Instruct
FALLBACK_MODEL=llama2
OLLAMA_HOST=http://localhost:11434
OLLAMA_TIMEOUT=60

# Increase Hugging Face router timeout (used by LLM_TIMEOUT_HF in pipeline)
LLM_TIMEOUT_HF=90

LANGCHAIN_API_KEY=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
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_PROJECT=pr-internal-radar-87
LANGCHAIN_TRACING=false
LANGCHAIN_VERBOSE=true
DISABLE_QWEN=1

# Disable local Transformers to avoid large downloads; use Hugging Face hosted Router instead
TRANSFORMERS_MODEL=
TRANSFORMERS_TRUST_REMOTE=0
TRANSFORMERS_DEVICE=-1
TRANSFORMERS_MAX_TOKENS=256

# Secondary HF Router / alternate inference settings cleared for now
HF_TOKEN_2=
HUGGINGFACE_ROUTER_MODEL_2=
HUGGINGFACE_ALTERNATE_URLS=
# Prefer a code-capable Hugging Face Router model (hosted) so we don't download weights locally.
# Change this to your preferred hosted model ID if you want a different model.
